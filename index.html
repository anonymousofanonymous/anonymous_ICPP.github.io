<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">

<!-- jQuery library -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

<!-- Latest compiled JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
  </head>
  <title>
  From RTL to CUDA: A GPU Acceleration Flow for RTL Simulation with Batch Stimulus
  </title>
    <div class="page-header text-center">
  <h1>
  From RTL to CUDA: A GPU Acceleration Flow for RTL Simulation with Batch Stimulus
  </h1>
  </div>
  <div class="row align-items-center">
    <div class="col-md-1">
    </div>
    <div class="col-md-10">
      <p>
   Dear Reviewers,</br>
 Thank you very much for your time and effort in reviewing our submission. Your review comments are very helpful for us to improve the manuscript. Please find our detailed response to your comments in the following sections. We believe this submission has properly addressed your comments.
      </p>
    </div>
  </div>
  <div class="row align-items-center">
    <div class="col-md-1">
    </div>
    <div class="col-md-10">
    <h3>Response to Reviewer #1</h3>
    <p>
    Thank you very much for your comments. We appreciate your time and effort in helping us improve the manuscript! Please see below our response.</br></br>

<b>Comment #1:</b> 
<i>Do you have a way to measure the absolute efficiency? Speedups are great but is this an efficient implementation or not?
</i></br></br>

<b>Response #1:</b> 
Thank you so much for the comment. Absolute efficiency can be measured by the latency needed to complete batch stimulus. Table 2 and Figure 12 provide some insight: When the number of stimulus is small (e.g., &#60;256), RTLflow does not benefit from much data parallelism and thus CPU-based Verilator is better. However, industrial simulators can easily call many thousands of stimulus where RTLflow (GPU) wins out. The break-even points can be observed in Table 2 for Spinal and NVDLA (256 and 1024 stimulus, respectively). Similar number is also observed in Figure 12 for riscv-mini. We will add the three break-even points in section 4.3 to highlight the absolute efficiency.
</br></br>
<div class="row align-items-center">
  <div class="col-md-4 col-sm-4 ">
  </div>
  <div class="col-md-8 col-sm-8 ">
<p><img class="text-center " srcset="img/bep.png" style="width: 55%;"></p>
  </div>
</div>
<div class="row align-items-center">
  <div class="col-md-5 col-sm-5 ">
  </div>
  <div class="col-md-7 col-sm-7 ">
<p><img class="text-center card-img-top" src="img/bep2.png" style="width: 35%;"></p>
  </div>
</div>


<b>Comment #2:</b> 
<i>What are the architectural bottlenecks?
</i></br></br>

<b>Response #2:</b> 
Thank you for the comment. RTLflow is primarily bottlenecked by the CPU-based call to set simulation input, i.e., <code>dut.set_inputs</code> in Listing 1. This is a constraint induced by the industrial convention where testbench drivers are typically written on CPU to read and set inputs from external files. However, as shown in Table 5, our pipeline can largely mitigate this challenge. We will revise Section 2.4.3 and Section 4.5 to highlight our explanations above.
</br></br>
<div class="row align-items-center">
  <div class="col-md-2 col-sm-2 ">
  </div>
  <div class="col-md-5 col-sm-5 ">
<p><img class="text-center card-img-top" src="img/code.png" style="width: 60%;"></p>
  </div>
  <div class="col-md-5 col-sm-5 ">
<p><img class="text-center card-img-top" src="img/table5.png" style="width: 65%;"></p>
  </div>
    <div class="col-md-1">
    </div>
</div>

    </p>
  </div>
    <div class="col-md-1">
    </div>
  </div>
  <div class="row align-items-center">

    <div class="col-md-1">
    </div>
    <div class="col-md-10">
    <h3>Response to Reviewer #2</h3>
    <p>
    Thank you very much for your comments. We appreciate your time and effort in helping us improve the manuscript! Please see below our response.</br></br>

<b>Comment #1:</b> 
<i>
A6000 is a single precision GPU card, there is no discussion about the specified precision and therefore, just by using a lower precision, the GPU hardware can demonstrate a much more efficient way, there is almost easily ×20 in terms of theoretical peak.
</i></br></br>
<b>Response #1:</b> 
Typical RTL simulation workloads do not involve any floating-point operations, i.e., all computations of RTLflow are integers. Unless using TensorCores or packed SIMD operations, which can be our future work, the performance is not affected by the precision issue. We will revise Section 3.1.2 to highlight RLTflow does not leverage single-precision optimizations.
</br></br>

<b>Comment #2:</b> 
<i>
The code has been refactorized on GPU, validation and accuracy must be demonstrated to check it compared to CPU version.
</i></br></br>
<b>Response #2:</b> 
Thank you so much for your comment. We validated all simulation results in Section 4 with the golden reference generated by Verilator. All signal outputs match and are accurate. We will add the explanation to Section 4.
</br></br>
<b>Comment #3:</b> 
<i>
 Figure 15 shall include the timing.
</i></br></br>
<b>Response #3:</b> 
Thank you for the comment. We will revise Figure 15 to include the timing. 
</br></br>
<div class="row align-items-center">
  <div class="col-md-5 col-sm-5 ">
  </div>
  <div class="col-md-5 col-sm-5 ">
<p><img class="text-center card-img-top" src="img/timeline.png" style="width: 60%;"></p>
  </div>
</div>
    </p>
    </div>
    <div class="col-md-1">
    </div>
  </div>
  <div class="row align-items-center">
    <div class="col-md-1">
    </div>
    <div class="col-md-10">
    <h3>Response to Reviewer #3</h3>
    <p>
    Thank you very much for your comments. We appreciate your time and effort in helping us improve the manuscript! Please see below our response.</br></br>

<b>Comment #1:</b> 
<i>
One of the proposed optimizations is storing data in different types of arrays that would fit the data type size. However there are different coalescing patterns depending on the amount of data and cache lines have a fixed amount. I am not convinced that the strategy will yield the expected results. Similarly when using those different data types for operation some casting will be required.
</i></br></br>
<b>Response #1:</b> 
Thank you so much for your comment. Yes, you are correct that different coalescing patterns depend on the number of stimulus. However, our transpliation does not impose any constraint on this and can be easily extended to handle different coalescing patterns through parameterization. For example, our pipeline scheduling algorithm partitions all stimulus into groups such that all stimulus within a group is simultaneously simulated on a GPU. We can ensure memory access is mostly coalesced by setting the proper group size (e.g., 256 or 1024 stimulus per group).
</br></br>

<b>Comment #2:</b> 
<i>
In listing 3 a uint16 operand is stored into an uint8 data type which seems incorrect.
</i></br></br>
<b>Response #2:</b> 
Thank you so much for your comment. The example code of Listing 2 and Listing 3 is not a typo since a hardware design is allowed to copy a wider value to a narrower target (truncation will be applied). Indeed, both Verilator and RTLflow internally perform type casting to resize variables, which we intend not to expose for clarity. The purpose of Listing 2 and Listing 3 is to demonstrate the difference between Verilator and RTLflow’s transpiled results. We use the same variable names as Figure 4, 6, and 7 to make variables consistent with Section 3.1. We will revise Section 3.1.3, Listing 2, and Listing 3 to re-mark the transpiled example code as pseudocode and add our explanations above to remove confusion.
</br></br>
<div class="row align-items-center">
  <div class="col-md-2 col-sm-2 ">
  </div>
  <div class="col-md-4 col-sm-4 ">
<p><img class="text-center card-img-top" src="img/code1.png" style="width: 75%;"></p>
  </div>
  <div class="col-md-4 col-sm-4 ">
<p><img class="text-center card-img-top" src="img/code2.png" style="width: 75%;"></p>
  </div>
</div>
<b>Comment #3:</b> 
<i>
In the experimental results section the -O2 is used for the compilation of RTLFlow. Why not using -O3?
</i></br></br>
<b>Response #3:</b> 
Thank you for the comment. We did not observe much performance difference between -O2 and -O3, but -O2 makes the compilation time faster (~3 minutes for -O2 and ~10 minutes for -O3). We will add the explanation to Section 4 and the Appendix.
</br></br>

<b>Comment #4:</b> 
<i>
While the paper shows graphically in fig 9 the theoretical benefit of CUDA Graphs an actual trace via the NSight System tool would have been more relevant to justify this and show the actual behaviour.
</i></br></br>
<b>Response #4:</b> 
Thank you for the comment. We will replace the graph with actual trace via the Nsight Systems to highlight the performance benefit of CUDA Graph.
</br></br>

    </p>
    </div>

    <div class="col-md-1">
    </div>
    </div>

</html>
